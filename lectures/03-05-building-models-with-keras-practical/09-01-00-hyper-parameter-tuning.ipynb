{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "297156c1-7deb-4c1d-b601-7a156476a408",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;font-size:22pt; font-weight:bold;color:white;border:solid black 1.5pt;background-color:#1e7263;\">\n",
    "    TensorBoard Callback Overview\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "131fa24a-96cf-4cc5-a2a7-945b355942c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================= #\n",
    "# Course: Deep Learning Complete Course (CS-501)\n",
    "# Author: Dr. Saad Laouadi\n",
    "# Institution: Quant Coding Versity Academy\n",
    "#\n",
    "# ==========================================================\n",
    "# Lesson: Understanding tensorboard callback\n",
    "#         Synthetic Data Example\n",
    "# ==========================================================\n",
    "# ## Learning Objectives\n",
    "# This example will enable you to:\n",
    "# 1. Understand the tensorboard callback\n",
    "# 2. Setup the environment for using tensorboard\n",
    "# =======================================================================\n",
    "#          Copyright Â© Dr. Saad Laouadi 2025\n",
    "# ======================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db04498-44c4-434f-8e8b-cec78e0419b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================ #\n",
    "#                         Environment Path Configuration                       #\n",
    "# ============================================================================ #\n",
    "#\n",
    "# Purpose:\n",
    "#   Configure the system PATH to use Python executables from the active virtual \n",
    "#   environment instead of global installations.\n",
    "#\n",
    "# Usage:\n",
    "#   1. First verify if configuration is needed by running: !which python\n",
    "#   2. If the output shows the global Python installation rather than your \n",
    "#      virtual environment, execute this configuration block\n",
    "#\n",
    "# Note:\n",
    "#   This configuration is typically only needed for JupyterLab Desktop or \n",
    "#   similar standalone installations. Web-based JupyterLab or properly \n",
    "#   configured environments should not require this adjustment.\n",
    "# ============================================================================ #\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "env_path = os.path.dirname(sys.executable)\n",
    "os.environ['PATH'] = f\"{env_path}:{os.environ['PATH']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c26ca844-8301-46b7-89da-e2dca2248ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "Author: Dr. Saad Laouadi\n",
      "\n",
      "Last updated: 2024-12-31\n",
      "\n",
      "Compiler    : Clang 14.0.6 \n",
      "OS          : Darwin\n",
      "Release     : 24.1.0\n",
      "Machine     : arm64\n",
      "Processor   : arm\n",
      "CPU cores   : 16\n",
      "Architecture: 64bit\n",
      "\n",
      "========================================================================\n",
      "Imported Packages and Their Versions:\n",
      "========================================================================\n",
      "sklearn   : 1.5.1\n",
      "keras     : 3.6.0\n",
      "matplotlib: 3.9.2\n",
      "tensorflow: 2.16.2\n",
      "seaborn   : 0.13.2\n",
      "numpy     : 1.26.4\n",
      "pandas    : 2.2.2\n",
      "\n",
      "========================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================== #\n",
    "#        Load Required Libraries\n",
    "# ==================================================== #\n",
    "\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import io\n",
    "\n",
    "\n",
    "# Disable Metal API Validation\n",
    "os.environ[\"METAL_DEVICE_WRAPPER_TYPE\"] = \"0\"  \n",
    "\n",
    "# import tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"=\"*72)\n",
    "\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Dr. Saad Laouadi\" -u -d -m\n",
    "\n",
    "print(\"=\"*72)\n",
    "print(\"Imported Packages and Their Versions:\")\n",
    "print(\"=\"*72)\n",
    "\n",
    "%watermark -iv\n",
    "print(\"=\"*72)\n",
    "\n",
    "# Global Config\n",
    "RANDOM_STATE = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1da7e80-7474-4533-a9e0-5365e16d9a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20245ebd-58eb-4e6a-bb40-6be3b1a276f9",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning in Deep Learning: A Comprehensive Guide\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Hyperparameter tuning is crucial for optimizing deep learning model performance. This guide covers various approaches, from basic grid search to advanced automated methods.\n",
    "\n",
    "## Key Hyperparameters\n",
    "\n",
    "### 1. Model Architecture\n",
    "- Number of layers\n",
    "- Units per layer\n",
    "- Activation functions\n",
    "- Layer types (Dense, CNN, RNN, etc.)\n",
    "\n",
    "### 2. Training Parameters\n",
    "- Learning rate\n",
    "- Batch size\n",
    "- Number of epochs\n",
    "- Optimizer choice\n",
    "\n",
    "### 3. Regularization Parameters\n",
    "- Dropout rate\n",
    "- L1/L2 regularization\n",
    "- Early stopping patience\n",
    "- Learning rate decay\n",
    "\n",
    "## Basic Implementation\n",
    "\n",
    "### 1. Grid Search\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def create_model(learning_rate=0.001, hidden_units=[64, 32], dropout_rate=0.2):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(hidden_units[0], activation='relu'),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(hidden_units[1], activation='relu'),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'hidden_units': [[32, 16], [64, 32], [128, 64]],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'epochs': [50, 100]\n",
    "}\n",
    "\n",
    "# Create model wrapper\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### 2. Random Search\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Define parameter distributions\n",
    "param_dist = {\n",
    "    'learning_rate': uniform(0.0001, 0.1),\n",
    "    'hidden_units': [[32, 16], [64, 32], [128, 64], [256, 128]],\n",
    "    'dropout_rate': uniform(0.1, 0.4),\n",
    "    'batch_size': randint(16, 256),\n",
    "    'epochs': randint(50, 200)\n",
    "}\n",
    "\n",
    "# Perform random search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "random_result = random_search.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "## Advanced Implementation\n",
    "\n",
    "### 1. Bayesian Optimization with Optuna\n",
    "\n",
    "```python\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to optimize\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 4)\n",
    "    hidden_units = []\n",
    "    for i in range(n_layers):\n",
    "        hidden_units.append(\n",
    "            trial.suggest_int(f'hidden_units_l{i}', 16, 256, log=True)\n",
    "        )\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 256, log=True)\n",
    "    \n",
    "    # Build model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(input_dim,)))\n",
    "    \n",
    "    for units in hidden_units:\n",
    "        model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=100,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    return history.history['val_accuracy'][-1]\n",
    "\n",
    "# Create study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "```\n",
    "\n",
    "### 2. Population Based Training (PBT)\n",
    "\n",
    "```python\n",
    "class PBTOptimizer:\n",
    "    def __init__(self, population_size=10, exploit_threshold=0.2):\n",
    "        self.population_size = population_size\n",
    "        self.exploit_threshold = exploit_threshold\n",
    "        self.population = []\n",
    "        \n",
    "    def initialize_population(self):\n",
    "        for _ in range(self.population_size):\n",
    "            hyperparams = {\n",
    "                'learning_rate': np.random.uniform(1e-4, 1e-2),\n",
    "                'hidden_units': [\n",
    "                    np.random.randint(32, 256),\n",
    "                    np.random.randint(16, 128)\n",
    "                ],\n",
    "                'dropout_rate': np.random.uniform(0.1, 0.5)\n",
    "            }\n",
    "            self.population.append({\n",
    "                'hyperparams': hyperparams,\n",
    "                'model': self.create_model(hyperparams),\n",
    "                'score': 0\n",
    "            })\n",
    "    \n",
    "    def exploit_and_explore(self):\n",
    "        # Sort population by score\n",
    "        self.population.sort(key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        # Replace bottom performers\n",
    "        cutoff = int(self.population_size * self.exploit_threshold)\n",
    "        for i in range(cutoff, self.population_size):\n",
    "            # Copy hyperparameters from top performer\n",
    "            donor_idx = np.random.randint(0, cutoff)\n",
    "            new_hyperparams = self.population[donor_idx]['hyperparams'].copy()\n",
    "            \n",
    "            # Perturb hyperparameters\n",
    "            new_hyperparams['learning_rate'] *= np.random.uniform(0.8, 1.2)\n",
    "            new_hyperparams['dropout_rate'] = np.clip(\n",
    "                new_hyperparams['dropout_rate'] * np.random.uniform(0.8, 1.2),\n",
    "                0.1, 0.5\n",
    "            )\n",
    "            \n",
    "            self.population[i] = {\n",
    "                'hyperparams': new_hyperparams,\n",
    "                'model': self.create_model(new_hyperparams),\n",
    "                'score': 0\n",
    "            }\n",
    "```\n",
    "\n",
    "### 3. Hyperband Implementation\n",
    "\n",
    "```python\n",
    "class Hyperband:\n",
    "    def __init__(self, max_iter=81, eta=3):\n",
    "        self.max_iter = max_iter\n",
    "        self.eta = eta\n",
    "        self.s_max = int(np.log(max_iter) / np.log(eta))\n",
    "        self.B = (self.s_max + 1) * max_iter\n",
    "        \n",
    "    def run_optimization(self, get_params_function, train_function):\n",
    "        for s in reversed(range(self.s_max + 1)):\n",
    "            n = int(np.ceil(int(self.B / self.max_iter / (s + 1)) * self.eta ** s))\n",
    "            r = self.max_iter * self.eta ** (-s)\n",
    "            \n",
    "            # Generate configurations\n",
    "            configs = [get_params_function() for _ in range(n)]\n",
    "            \n",
    "            for i in range(s + 1):\n",
    "                n_i = n * self.eta ** (-i)\n",
    "                r_i = r * self.eta ** i\n",
    "                \n",
    "                # Run each configuration for r_i iterations\n",
    "                val_losses = []\n",
    "                for config in configs:\n",
    "                    loss = train_function(config, r_i)\n",
    "                    val_losses.append(loss)\n",
    "                \n",
    "                # Keep top 1/eta configurations\n",
    "                indices = np.argsort(val_losses)\n",
    "                n_survivors = int(n_i / self.eta)\n",
    "                configs = [configs[i] for i in indices[:n_survivors]]\n",
    "```\n",
    "\n",
    "## Advanced Techniques\n",
    "\n",
    "### 1. Automated Hyperparameter Tuning Pipeline\n",
    "\n",
    "```python\n",
    "class AutoHyperTuner:\n",
    "    def __init__(self, strategy='optuna', n_trials=100):\n",
    "        self.strategy = strategy\n",
    "        self.n_trials = n_trials\n",
    "        self.best_params = None\n",
    "        self.best_score = None\n",
    "        \n",
    "    def optimize(self, X_train, y_train, X_val, y_val):\n",
    "        if self.strategy == 'optuna':\n",
    "            study = optuna.create_study(direction='maximize')\n",
    "            study.optimize(\n",
    "                lambda trial: self._objective(trial, X_train, y_train, X_val, y_val),\n",
    "                n_trials=self.n_trials\n",
    "            )\n",
    "            self.best_params = study.best_params\n",
    "            self.best_score = study.best_value\n",
    "            \n",
    "        elif self.strategy == 'pbt':\n",
    "            pbt = PBTOptimizer(population_size=10)\n",
    "            self.best_params, self.best_score = pbt.optimize(\n",
    "                X_train, y_train, X_val, y_val\n",
    "            )\n",
    "    \n",
    "    def _objective(self, trial, X_train, y_train, X_val, y_val):\n",
    "        params = self._get_trial_params(trial)\n",
    "        model = self._build_model(params)\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=params['epochs'],\n",
    "            batch_size=params['batch_size'],\n",
    "            verbose=0\n",
    "        )\n",
    "        return history.history['val_accuracy'][-1]\n",
    "```\n",
    "\n",
    "### 2. Multi-Objective Optimization\n",
    "\n",
    "```python\n",
    "def multi_objective_optimization(trial):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True),\n",
    "        'n_layers': trial.suggest_int('n_layers', 1, 4),\n",
    "        'batch_size': trial.suggest_int('batch_size', 16, 256, log=True)\n",
    "    }\n",
    "    \n",
    "    model = build_model(params)\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=100,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Return multiple objectives\n",
    "    return [\n",
    "        history.history['val_accuracy'][-1],  # Maximize accuracy\n",
    "        -history.history['val_loss'][-1],     # Minimize loss\n",
    "        -get_model_size(model)                # Minimize model size\n",
    "    ]\n",
    "\n",
    "study = optuna.create_study(directions=['maximize', 'maximize', 'maximize'])\n",
    "study.optimize(multi_objective_optimization, n_trials=100)\n",
    "```\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "### 1. Parameter Space Definition\n",
    "\n",
    "```python\n",
    "def define_parameter_space():\n",
    "    return {\n",
    "        'continuous': {\n",
    "            'learning_rate': (1e-5, 1e-1, 'log'),\n",
    "            'dropout_rate': (0.1, 0.5, 'uniform'),\n",
    "            'weight_decay': (1e-6, 1e-3, 'log')\n",
    "        },\n",
    "        'discrete': {\n",
    "            'batch_size': ([16, 32, 64, 128, 256], 'choice'),\n",
    "            'hidden_units': ([32, 64, 128, 256, 512], 'choice')\n",
    "        },\n",
    "        'categorical': {\n",
    "            'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
    "            'activation': ['relu', 'elu', 'selu']\n",
    "        }\n",
    "    }\n",
    "```\n",
    "\n",
    "### 2. Cross-Validation Strategy\n",
    "\n",
    "```python\n",
    "def cross_validated_tuning(model_fn, param_grid, X, y, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "    results = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        X_train_fold = X[train_idx]\n",
    "        y_train_fold = y[train_idx]\n",
    "        X_val_fold = X[val_idx]\n",
    "        y_val_fold = y[val_idx]\n",
    "        \n",
    "        for params in ParameterGrid(param_grid):\n",
    "            model = model_fn(**params)\n",
    "            history = model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                validation_data=(X_val_fold, y_val_fold),\n",
    "                verbose=0\n",
    "            )\n",
    "            results.append({\n",
    "                'fold': fold,\n",
    "                'params': params,\n",
    "                'score': history.history['val_accuracy'][-1]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "```\n",
    "\n",
    "## Resource Management\n",
    "\n",
    "### 1. Memory Efficient Tuning\n",
    "\n",
    "```python\n",
    "class MemoryEfficientTuner:\n",
    "    def __init__(self):\n",
    "        self.current_model = None\n",
    "        \n",
    "    def evaluate_params(self, params):\n",
    "        # Clear previous model\n",
    "        if self.current_model is not None:\n",
    "            del self.current_model\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "        \n",
    "        # Build and evaluate new model\n",
    "        self.current_model = self.build_model(params)\n",
    "        return self.train_and_evaluate()\n",
    "```\n",
    "\n",
    "### 2. Distributed Hyperparameter Tuning\n",
    "\n",
    "```python\n",
    "class DistributedTuner:\n",
    "    def __init__(self, n_workers=4):\n",
    "        self.n_workers = n_workers\n",
    "        \n",
    "    def parallel_optimize(self, param_space):\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=self.n_workers) as executor:\n",
    "            futures = []\n",
    "            for params in self.generate_params(param_space):\n",
    "                futures.append(\n",
    "                    executor.submit(self.evaluate_single_config, params)\n",
    "                )\n",
    "            \n",
    "            results = []\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                results.append(future.result())\n",
    "                \n",
    "        return self.aggregate_results(results)\n",
    "```\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Effective hyperparameter tuning is crucial for deep learning success. Key points:\n",
    "\n",
    "1. Choose appropriate search strategy based on computational resources\n",
    "2. Define meaningful parameter spaces\n",
    "3. Use cross-validation for robust evaluation\n",
    "4. Consider multi-objective optimization when relevant\n",
    "5. Implement proper resource management\n",
    "6. Monitor and analyze results carefully\n",
    "\n",
    "For more advanced techniques and implementations, refer to the documentation of specialized libraries like Optuna, Ray Tune, and Keras Tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42e1345-e3e6-4260-9240-0b90b3cc8f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fff4d09d-55d6-4913-9d9b-f2ec2452f335",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 12:18:29.890 python[22420:29462828] Metal API Validation Enabled\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras.wrappers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscikit_learn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasClassifier\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.wrappers'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844fd9c2-a3bb-4650-a8db-f97d8f17ef38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF-GPU:2.16",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
