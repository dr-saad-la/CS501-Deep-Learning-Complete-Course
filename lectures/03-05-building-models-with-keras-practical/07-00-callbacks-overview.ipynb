{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f09c921d-6edf-498f-9f61-ff30a7e11fc8",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;font-size:22pt; font-weight:bold;color:white;border:solid black 1.5pt;background-color:#1e7263;\">\n",
    "    Callbacks Overview\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4823a9d6-5e35-43a7-b79a-a8e19efa105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================= #\n",
    "# Course: Deep Learning Complete Course (CS-501)\n",
    "# Author: Dr. Saad Laouadi\n",
    "# Institution: Quant Coding Versity Academy\n",
    "#\n",
    "# ==========================================================\n",
    "# Lesson: Understanding Callbacks in Keras API\n",
    "#         \n",
    "# ==========================================================\n",
    "# ## Learning Objectives\n",
    "# This guide will enable you to:\n",
    "# 1. \n",
    "# =======================================================================\n",
    "#          Copyright Â© Dr. Saad Laouadi 2024\n",
    "# ======================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f35198ce-760b-4515-a2e7-a162a3f19291",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "Author: Dr. Saad Laouadi\n",
      "\n",
      "Last updated: 2025-01-04\n",
      "\n",
      "Compiler    : Clang 14.0.6 \n",
      "OS          : Darwin\n",
      "Release     : 24.1.0\n",
      "Machine     : arm64\n",
      "Processor   : arm\n",
      "CPU cores   : 16\n",
      "Architecture: 64bit\n",
      "\n",
      "========================================================================\n",
      "Imported Packages and Their Versions:\n",
      "========================================================================\n",
      "tensorflow: 2.16.2\n",
      "keras     : 3.6.0\n",
      "\n",
      "========================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================== #\n",
    "#        Load Required Libraries\n",
    "# ==================================================== #\n",
    "\n",
    "import os  \n",
    "\n",
    "# Disable Metal API Validation\n",
    "os.environ[\"METAL_DEVICE_WRAPPER_TYPE\"] = \"0\"  \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "print(\"=\"*72)\n",
    "\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Dr. Saad Laouadi\" -u -d -m\n",
    "\n",
    "print(\"=\"*72)\n",
    "print(\"Imported Packages and Their Versions:\")\n",
    "print(\"=\"*72)\n",
    "\n",
    "%watermark -iv\n",
    "print(\"=\"*72)\n",
    "\n",
    "# Global Config\n",
    "RANDOM_STATE = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f60113a-3c3d-42c3-9aef-a785d963d4c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 ==> BackupAndRestore\n",
      "02 ==> CSVLogger\n",
      "03 ==> Callback\n",
      "04 ==> CallbackList\n",
      "05 ==> EarlyStopping\n",
      "06 ==> History\n",
      "07 ==> LambdaCallback\n",
      "08 ==> LearningRateScheduler\n",
      "09 ==> ModelCheckpoint\n",
      "10 ==> ProgbarLogger\n",
      "11 ==> ReduceLROnPlateau\n",
      "12 ==> RemoteMonitor\n",
      "13 ==> SwapEMAWeights\n",
      "14 ==> TensorBoard\n",
      "15 ==> TerminateOnNaN\n"
     ]
    }
   ],
   "source": [
    "# List the available callbacks in Keras API\n",
    "for ind, callback in enumerate(dir(callbacks), 1):\n",
    "    if not callback.startswith('_'):\n",
    "        print(f\"{ind:0>2d} ==> {callback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88479a77-1d42-4c9d-92a3-911c05e7dd10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbaseline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstart_from_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Stop training when a monitored metric has stopped improving.\n",
       "\n",
       "Assuming the goal of a training is to minimize the loss. With this, the\n",
       "metric to be monitored would be `'loss'`, and mode would be `'min'`. A\n",
       "`model.fit()` training loop will check at end of every epoch whether\n",
       "the loss is no longer decreasing, considering the `min_delta` and\n",
       "`patience` if applicable. Once it's found no longer decreasing,\n",
       "`model.stop_training` is marked True and the training terminates.\n",
       "\n",
       "The quantity to be monitored needs to be available in `logs` dict.\n",
       "To make it so, pass the loss or metrics at `model.compile()`.\n",
       "\n",
       "Args:\n",
       "    monitor: Quantity to be monitored. Defaults to `\"val_loss\"`.\n",
       "    min_delta: Minimum change in the monitored quantity to qualify as an\n",
       "        improvement, i.e. an absolute change of less than min_delta, will\n",
       "        count as no improvement. Defaults to `0`.\n",
       "    patience: Number of epochs with no improvement after which training will\n",
       "        be stopped. Defaults to `0`.\n",
       "    verbose: Verbosity mode, 0 or 1. Mode 0 is silent, and mode 1 displays\n",
       "        messages when the callback takes an action. Defaults to `0`.\n",
       "    mode: One of `{\"auto\", \"min\", \"max\"}`. In `min` mode, training will stop\n",
       "        when the quantity monitored has stopped decreasing; in `\"max\"` mode\n",
       "        it will stop when the quantity monitored has stopped increasing; in\n",
       "        `\"auto\"` mode, the direction is automatically inferred from the name\n",
       "        of the monitored quantity. Defaults to `\"auto\"`.\n",
       "    baseline: Baseline value for the monitored quantity. If not `None`,\n",
       "        training will stop if the model doesn't show improvement over the\n",
       "        baseline. Defaults to `None`.\n",
       "    restore_best_weights: Whether to restore model weights from the epoch\n",
       "        with the best value of the monitored quantity. If `False`, the model\n",
       "        weights obtained at the last step of training are used. An epoch\n",
       "        will be restored regardless of the performance relative to the\n",
       "        `baseline`. If no epoch improves on `baseline`, training will run\n",
       "        for `patience` epochs and restore weights from the best epoch in\n",
       "        that set. Defaults to `False`.\n",
       "    start_from_epoch: Number of epochs to wait before starting to monitor\n",
       "        improvement. This allows for a warm-up period in which no\n",
       "        improvement is expected and thus training will not be stopped.\n",
       "        Defaults to `0`.\n",
       "\n",
       "Example:\n",
       "\n",
       ">>> callback = keras.callbacks.EarlyStopping(monitor='loss',\n",
       "...                                               patience=3)\n",
       ">>> # This callback will stop the training when there is no improvement in\n",
       ">>> # the loss for three consecutive epochs.\n",
       ">>> model = keras.models.Sequential([keras.layers.Dense(10)])\n",
       ">>> model.compile(keras.optimizers.SGD(), loss='mse')\n",
       ">>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n",
       "...                     epochs=10, batch_size=1, callbacks=[callback],\n",
       "...                     verbose=0)\n",
       ">>> len(history.history['loss'])  # Only 4 epochs are run.\n",
       "4\n",
       "\u001b[0;31mFile:\u001b[0m           /opt/homebrew/Caskroom/miniforge/base/envs/tf-gpu/lib/python3.11/site-packages/keras/src/callbacks/early_stopping.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the help for Early Stopping Callback\n",
    "?tf.keras.callbacks.EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccfa681b-545d-4ebf-9f33-62c62781a213",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msave_weights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msave_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minitial_value_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Callback to save the Keras model or model weights at some frequency.\n",
       "\n",
       "`ModelCheckpoint` callback is used in conjunction with training using\n",
       "`model.fit()` to save a model or weights (in a checkpoint file) at some\n",
       "interval, so the model or weights can be loaded later to continue the\n",
       "training from the state saved.\n",
       "\n",
       "A few options this callback provides include:\n",
       "\n",
       "- Whether to only keep the model that has achieved the \"best performance\" so\n",
       "  far, or whether to save the model at the end of every epoch regardless of\n",
       "  performance.\n",
       "- Definition of \"best\"; which quantity to monitor and whether it should be\n",
       "  maximized or minimized.\n",
       "- The frequency it should save at. Currently, the callback supports saving\n",
       "  at the end of every epoch, or after a fixed number of training batches.\n",
       "- Whether only weights are saved, or the whole model is saved.\n",
       "\n",
       "Example:\n",
       "\n",
       "```python\n",
       "model.compile(loss=..., optimizer=...,\n",
       "              metrics=['accuracy'])\n",
       "\n",
       "EPOCHS = 10\n",
       "checkpoint_filepath = '/tmp/ckpt/checkpoint.model.keras'\n",
       "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
       "    filepath=checkpoint_filepath,\n",
       "    monitor='val_accuracy',\n",
       "    mode='max',\n",
       "    save_best_only=True)\n",
       "\n",
       "# Model is saved at the end of every epoch, if it's the best seen so far.\n",
       "model.fit(epochs=EPOCHS, callbacks=[model_checkpoint_callback])\n",
       "\n",
       "# The model (that are considered the best) can be loaded as -\n",
       "keras.models.load_model(checkpoint_filepath)\n",
       "\n",
       "# Alternatively, one could checkpoint just the model weights as -\n",
       "checkpoint_filepath = '/tmp/ckpt/checkpoint.weights.h5'\n",
       "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
       "    filepath=checkpoint_filepath,\n",
       "    save_weights_only=True,\n",
       "    monitor='val_accuracy',\n",
       "    mode='max',\n",
       "    save_best_only=True)\n",
       "\n",
       "# Model weights are saved at the end of every epoch, if it's the best seen\n",
       "# so far.\n",
       "model.fit(epochs=EPOCHS, callbacks=[model_checkpoint_callback])\n",
       "\n",
       "# The model weights (that are considered the best) can be loaded as -\n",
       "model.load_weights(checkpoint_filepath)\n",
       "```\n",
       "\n",
       "Args:\n",
       "    filepath: string or `PathLike`, path to save the model file.\n",
       "        `filepath` can contain named formatting options,\n",
       "        which will be filled the value of `epoch` and keys in `logs`\n",
       "        (passed in `on_epoch_end`).\n",
       "        The `filepath` name needs to end with `\".weights.h5\"` when\n",
       "        `save_weights_only=True` or should end with `\".keras\"` when\n",
       "        checkpoint saving the whole model (default).\n",
       "        For example:\n",
       "        if `filepath` is `\"{epoch:02d}-{val_loss:.2f}.keras\"`, then the\n",
       "        model checkpoints will be saved with the epoch number and the\n",
       "        validation loss in the filename. The directory of the filepath\n",
       "        should not be reused by any other callbacks to avoid conflicts.\n",
       "    monitor: The metric name to monitor. Typically the metrics are set by\n",
       "        the `Model.compile` method. Note:\n",
       "        * Prefix the name with `\"val_\"` to monitor validation metrics.\n",
       "        * Use `\"loss\"` or `\"val_loss\"` to monitor the model's total loss.\n",
       "        * If you specify metrics as strings, like `\"accuracy\"`, pass the\n",
       "            same string (with or without the `\"val_\"` prefix).\n",
       "        * If you pass `metrics.Metric` objects, `monitor` should be set to\n",
       "            `metric.name`\n",
       "        * If you're not sure about the metric names you can check the\n",
       "            contents of the `history.history` dictionary returned by\n",
       "            `history = model.fit()`\n",
       "        * Multi-output models set additional prefixes on the metric names.\n",
       "    verbose: Verbosity mode, 0 or 1. Mode 0 is silent, and mode 1\n",
       "        displays messages when the callback takes an action.\n",
       "    save_best_only: if `save_best_only=True`, it only saves when the model\n",
       "        is considered the \"best\" and the latest best model according to the\n",
       "        quantity monitored will not be overwritten. If `filepath` doesn't\n",
       "        contain formatting options like `{epoch}` then `filepath` will be\n",
       "        overwritten by each new better model.\n",
       "    mode: one of {`\"auto\"`, `\"min\"`, `\"max\"`}. If `save_best_only=True`, the\n",
       "        decision to overwrite the current save file is made based on either\n",
       "        the maximization or the minimization of the monitored quantity.\n",
       "        For `val_acc`, this should be `\"max\"`, for `val_loss` this should be\n",
       "        `\"min\"`, etc. In `\"auto\"` mode, the mode is set to `\"max\"` if the\n",
       "        quantities monitored are `\"acc\"` or start with `\"fmeasure\"` and are\n",
       "        set to `\"min\"` for the rest of the quantities.\n",
       "    save_weights_only: if `True`, then only the model's weights will be\n",
       "        saved (`model.save_weights(filepath)`), else the full model is\n",
       "        saved (`model.save(filepath)`).\n",
       "    save_freq: `\"epoch\"` or integer. When using `\"epoch\"`, the callback\n",
       "        saves the model after each epoch. When using integer, the callback\n",
       "        saves the model at end of this many batches. If the `Model` is\n",
       "        compiled with `steps_per_execution=N`, then the saving criteria will\n",
       "        be checked every Nth batch. Note that if the saving isn't aligned to\n",
       "        epochs, the monitored metric may potentially be less reliable (it\n",
       "        could reflect as little as 1 batch, since the metrics get reset\n",
       "        every epoch). Defaults to `\"epoch\"`.\n",
       "    initial_value_threshold: Floating point initial \"best\" value of the\n",
       "        metric to be monitored. Only applies if `save_best_value=True`. Only\n",
       "        overwrites the model weights already saved if the performance of\n",
       "        current model is better than this value.\n",
       "\u001b[0;31mFile:\u001b[0m           /opt/homebrew/Caskroom/miniforge/base/envs/tf-gpu/lib/python3.11/site-packages/keras/src/callbacks/model_checkpoint.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the help for ModelCheckpoint Callback\n",
    "?tf.keras.callbacks.ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf3faac-db33-441b-b8df-7e66117bfb92",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92693fc5-c617-4008-8b76-df301b1a0d76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcooldown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Reduce learning rate when a metric has stopped improving.\n",
       "\n",
       "Models often benefit from reducing the learning rate by a factor\n",
       "of 2-10 once learning stagnates. This callback monitors a\n",
       "quantity and if no improvement is seen for a 'patience' number\n",
       "of epochs, the learning rate is reduced.\n",
       "\n",
       "Example:\n",
       "\n",
       "```python\n",
       "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
       "                              patience=5, min_lr=0.001)\n",
       "model.fit(x_train, y_train, callbacks=[reduce_lr])\n",
       "```\n",
       "\n",
       "Args:\n",
       "    monitor: String. Quantity to be monitored.\n",
       "    factor: Float. Factor by which the learning rate will be reduced.\n",
       "        `new_lr = lr * factor`.\n",
       "    patience: Integer. Number of epochs with no improvement after which\n",
       "        learning rate will be reduced.\n",
       "    verbose: Integer. 0: quiet, 1: update messages.\n",
       "    mode: String. One of `{'auto', 'min', 'max'}`. In `'min'` mode,\n",
       "        the learning rate will be reduced when the\n",
       "        quantity monitored has stopped decreasing; in `'max'` mode it will\n",
       "        be reduced when the quantity monitored has stopped increasing; in\n",
       "        `'auto'` mode, the direction is automatically inferred from the name\n",
       "        of the monitored quantity.\n",
       "    min_delta: Float. Threshold for measuring the new optimum, to only focus\n",
       "        on significant changes.\n",
       "    cooldown: Integer. Number of epochs to wait before resuming normal\n",
       "        operation after the learning rate has been reduced.\n",
       "    min_lr: Float. Lower bound on the learning rate.\n",
       "\u001b[0;31mFile:\u001b[0m           /opt/homebrew/Caskroom/miniforge/base/envs/tf-gpu/lib/python3.11/site-packages/keras/src/callbacks/reduce_lr_on_plateau.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the help for ReduceOnPlateau Callback\n",
    "?callbacks.ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf99910-e09b-4ff0-a76a-e2d1864ab5ce",
   "metadata": {},
   "source": [
    "# Complete List of Keras Callbacks\n",
    "\n",
    "## Model Checkpointing and Saving\n",
    "\n",
    "### 1. ModelCheckpoint\n",
    "- **Purpose**: Saves model weights during training\n",
    "- **Key Parameters**:\n",
    "  - `filepath`: Path to save the model file\n",
    "  - `monitor`: Metric to monitor\n",
    "  - `save_best_only`: Only save when model improves\n",
    "  - `save_weights_only`: Save only weights vs entire model\n",
    "  - `mode`: 'auto', 'min', or 'max'\n",
    "  - `save_freq`: 'epoch' or integer batch interval\n",
    "\n",
    "### 2. BackupAndRestore\n",
    "- **Purpose**: Enables fault tolerance in training\n",
    "- **Key Parameters**:\n",
    "  - `backup_dir`: Directory for backup files\n",
    "  - `save_freq`: Frequency of backups\n",
    "  - `delete_checkpoint`: Whether to delete backup after restoration\n",
    "\n",
    "## Training Optimization\n",
    "\n",
    "### 3. EarlyStopping\n",
    "- **Purpose**: Stops training when model stops improving\n",
    "- **Key Parameters**:\n",
    "  - `monitor`: Metric to monitor\n",
    "  - `patience`: Number of epochs to wait\n",
    "  - `restore_best_weights`: Revert to best weights\n",
    "  - `mode`: 'auto', 'min', or 'max'\n",
    "  - `min_delta`: Minimum change to qualify as improvement\n",
    "\n",
    "### 4. ReduceLROnPlateau\n",
    "- **Purpose**: Reduces learning rate when metrics plateau\n",
    "- **Key Parameters**:\n",
    "  - `monitor`: Metric to monitor\n",
    "  - `factor`: Factor to reduce learning rate by\n",
    "  - `patience`: Number of epochs to wait\n",
    "  - `min_lr`: Minimum learning rate\n",
    "  - `cooldown`: Epochs to wait before resuming\n",
    "\n",
    "### 5. LearningRateScheduler\n",
    "- **Purpose**: Dynamically adjusts learning rate\n",
    "- **Key Parameters**:\n",
    "  - `schedule`: Function that takes epoch and returns LR\n",
    "  - `verbose`: Logging verbosity\n",
    "\n",
    "## Monitoring and Logging\n",
    "\n",
    "### 6. TensorBoard\n",
    "- **Purpose**: Enables TensorBoard visualization\n",
    "- **Key Parameters**:\n",
    "  - `log_dir`: Directory to save logs\n",
    "  - `histogram_freq`: Frequency of histogram updates\n",
    "  - `write_graph`: Whether to visualize graph\n",
    "  - `write_images`: Log image summaries\n",
    "  - `update_freq`: 'batch', 'epoch', or integer\n",
    "\n",
    "### 7. CSVLogger\n",
    "- **Purpose**: Logs metrics to CSV file\n",
    "- **Key Parameters**:\n",
    "  - `filename`: Path to CSV file\n",
    "  - `separator`: Column separator\n",
    "  - `append`: Whether to append to existing file\n",
    "\n",
    "### 8. ProgbarLogger\n",
    "- **Purpose**: Prints metrics to stdout\n",
    "- **Key Parameters**:\n",
    "  - `count_mode`: 'samples' or 'steps'\n",
    "  - `stateful_metrics`: Metrics that shouldn't be averaged\n",
    "\n",
    "## Specialized Callbacks\n",
    "\n",
    "### 9. TerminateOnNaN\n",
    "- **Purpose**: Terminates training if loss becomes NaN\n",
    "- **Key Parameters**: None\n",
    "\n",
    "### 10. RemoteMonitor\n",
    "- **Purpose**: Sends metrics to remote server\n",
    "- **Key Parameters**:\n",
    "  - `root`: Server root URL\n",
    "  - `path`: Path for metrics\n",
    "  - `field`: JSON field name\n",
    "  - `headers`: Custom HTTP headers\n",
    "\n",
    "### 11. LambdaCallback\n",
    "- **Purpose**: Creates simple custom callbacks\n",
    "- **Key Parameters**:\n",
    "  - `on_epoch_begin`\n",
    "  - `on_epoch_end`\n",
    "  - `on_batch_begin`\n",
    "  - `on_batch_end`\n",
    "  - `on_train_begin`\n",
    "  - `on_train_end`\n",
    "\n",
    "## Base Callback Methods\n",
    "\n",
    "All callbacks inherit from the base Callback class and can implement these methods:\n",
    "\n",
    "```python\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None)\n",
    "    def on_train_end(self, logs=None)\n",
    "    def on_epoch_begin(self, epoch, logs=None)\n",
    "    def on_epoch_end(self, epoch, logs=None)\n",
    "    def on_test_begin(self, logs=None)\n",
    "    def on_test_end(self, logs=None)\n",
    "    def on_predict_begin(self, logs=None)\n",
    "    def on_predict_end(self, logs=None)\n",
    "    def on_batch_begin(self, batch, logs=None)\n",
    "    def on_batch_end(self, batch, logs=None)\n",
    "```\n",
    "\n",
    "## Common Usage Pattern\n",
    "\n",
    "```python\n",
    "# Combining multiple callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        'best_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=5\n",
    "    ),\n",
    "    CSVLogger('training.log'),\n",
    "    TensorBoard(log_dir='./logs')\n",
    "]\n",
    "\n",
    "# Using callbacks in model training\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "```\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Order Matters**: Place critical callbacks (like EarlyStopping) before monitoring callbacks\n",
    "\n",
    "2. **Resource Management**: Be mindful of disk space when using ModelCheckpoint and TensorBoard\n",
    "\n",
    "3. **Monitoring**: Always include at least one monitoring callback (CSVLogger or TensorBoard)\n",
    "\n",
    "4. **Fault Tolerance**: Use BackupAndRestore for long training sessions\n",
    "\n",
    "5. **Custom Metrics**: When using custom metrics, ensure they're properly logged in callbacks\n",
    "\n",
    "## Advanced Usage Tips\n",
    "\n",
    "1. **Chaining Callbacks**: Multiple callbacks can work together:\n",
    "```python\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=10),\n",
    "    ReduceLROnPlateau(patience=5),  # Tries reducing LR before stopping\n",
    "]\n",
    "```\n",
    "\n",
    "2. **Custom Checkpoint Naming**:\n",
    "```python\n",
    "ModelCheckpoint(\n",
    "    filepath='model_{epoch:02d}-{val_loss:.2f}.h5',\n",
    "    save_best_only=True\n",
    ")\n",
    "```\n",
    "\n",
    "3. **Dynamic Learning Rate Scheduling**:\n",
    "```python\n",
    "def schedule(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "LearningRateScheduler(schedule)\n",
    "```\n",
    "\n",
    "4. **Custom Training Monitoring**:\n",
    "```python\n",
    "class MetricsHistory(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if logs.get('accuracy') > 0.95:\n",
    "            print('Reached 95% accuracy, stopping training.')\n",
    "            self.model.stop_training = True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af78c92-f0fc-40a5-977d-3fd76e75a037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF-GPU:2.16",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
