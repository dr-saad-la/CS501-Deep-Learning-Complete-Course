{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50026306-2640-4e16-9bf1-bec66bf41320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ======================================================================= #\n",
    "# Course: Deep Learning Complete Course (CS-501)\n",
    "# Author: Dr. Saad Laouadi\n",
    "# Lesson: Deep Learning Regression Tutorial\n",
    "#\n",
    "# Description: Training Linear Regression with Keras 3 API\n",
    "#    \"\"\"\n",
    "#    Project Description:\n",
    "#    ------------------\n",
    "#    This notebook demonstrates how to build a deep learning regression model using TensorFlow/Keras.\n",
    "#    We'll generate synthetic data using scikit-learn, then build, train, and evaluate a neural network\n",
    "#    for regression tasks. This tutorial is designed for educational purposes to help understand the\n",
    "#    complete workflow of creating deep learning models for regression problems.\n",
    "#\n",
    "#    Objectives:\n",
    "#    ----------\n",
    "#    1. Learn how to generate synthetic regression data\n",
    "#    2. Understand deep learning model architecture for regression\n",
    "#    3. Learn the proper steps for data preprocessing\n",
    "#    4. Build and compile a neural network using Keras\n",
    "#    5. Train and evaluate the model's performance\n",
    "#    6. Visualize the results and model predictions\n",
    "#    \"\"\"\n",
    "# =======================================================================\n",
    "#.          Copyright Â© Dr. Saad Laouadi\n",
    "# ======================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47f0ec76-332c-40d3-a330-446845703d34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "Author: Dr. Saad Laouadi\n",
      "\n",
      "Last updated: 2024-11-29\n",
      "\n",
      "Compiler    : Clang 14.0.6 \n",
      "OS          : Darwin\n",
      "Release     : 24.1.0\n",
      "Machine     : arm64\n",
      "Processor   : arm\n",
      "CPU cores   : 16\n",
      "Architecture: 64bit\n",
      "\n",
      "========================================================================\n",
      "Imported Packages and Their Versions:\n",
      "========================================================================\n",
      "numpy     : 1.26.4\n",
      "tensorflow: 2.16.2\n",
      "sys       : 3.11.10 (main, Oct  3 2024, 02:26:51) [Clang 14.0.6 ]\n",
      "matplotlib: 3.9.2\n",
      "pandas    : 2.2.2\n",
      "sklearn   : 1.5.1\n",
      "keras     : 3.6.0\n",
      "\n",
      "========================================================================\n"
     ]
    }
   ],
   "source": [
    "# 1. Environment Setup\n",
    "# ------------------\n",
    "import os  \n",
    "import sys \n",
    "from pathlib import Path\n",
    "\n",
    "# Disable Metal API Validation\n",
    "os.environ[\"METAL_DEVICE_WRAPPER_TYPE\"] = \"0\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "print(\"=\"*72)\n",
    "\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Dr. Saad Laouadi\" -u -d -m\n",
    "\n",
    "print(\"=\"*72)\n",
    "print(\"Imported Packages and Their Versions:\")\n",
    "print(\"=\"*72)\n",
    "\n",
    "%watermark -iv\n",
    "print(\"=\"*72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd796176-06c0-4132-9f0b-59c3537f5214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "180f815d-ed32-4d26-b2de-b803dc8fbaf9",
   "metadata": {},
   "source": [
    "## Scaling and Normalization\n",
    "\n",
    "**Scaling/normalization*** is generally necessary for neural networks, even with random data, for several important reasons:\n",
    "\n",
    "1. **Gradient Descent Efficiency**: Neural networks train better when all input features are on a similar scale. Without scaling:\n",
    "    - Features with larger values could dominate the training process\n",
    "    - The gradient descent algorithm may converge much slower\n",
    "    - You might need to use a very small learning rate to prevent overshooting\n",
    "\n",
    "\n",
    "2. **Neural Network Sensitivity**: Neural networks are sensitive to the scale of input features because:\n",
    "    - The weights are randomly initialized in a small range (typically between -1 and 1)\n",
    "    - Activation functions like sigmoid or tanh have limited output ranges\n",
    "    - **Large input values can cause**:\n",
    "        - Saturation of activation functions\n",
    "        - Exploding gradients\n",
    "        - Numerical instability\n",
    "\n",
    "Even though we're generating random data, the values from make_regression might be in different scales. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fd538ec-ec11-427f-822e-e06a30c987c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X range: -3.72556347602681 to 3.2814121865882973\n"
     ]
    }
   ],
   "source": [
    "X, y = make_regression(n_samples=1000, n_features=1)\n",
    "print(\"X range:\", X.min(), \"to\", X.max())\n",
    "# print(\"y range:\", y.min(), \"to\", y.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e41e8f-8d0a-470c-94a9-2a66c7cdc735",
   "metadata": {},
   "source": [
    "## Does The Target Variable Need to be Scaled?\n",
    "\n",
    "We don't necessarily need to scale the target variable (Y) in this case. Good catch! Let me explain why:\n",
    "\n",
    "1. Input features (X) scaling is crucial because:\n",
    "    - It affects the weight updates during gradient descent\n",
    "    - Helps prevent neuron saturation\n",
    "    - Makes training more stable and efficient\n",
    "\n",
    "\n",
    "2. Target variable (Y) scaling is optional and depends on the use case:\n",
    "    - For regression, keeping Y in its original scale:\n",
    "    - Makes predictions more interpretable\n",
    "    - Eliminates need for inverse transformation\n",
    "    - Helps in directly calculating metrics like MSE in the original scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c802a3a3-d393-462b-a90d-192d60437b4e",
   "metadata": {},
   "source": [
    "## Splitting before Scaling\n",
    "\n",
    "- We should always split the data before scaling to prevent data leakage. The scaler should only learn from the training data. Here's the corrected order:\n",
    "\n",
    "- Deep Learning Regression Tutorial with Correct Split and Scaling:\n",
    "- This approach is better because:\n",
    "    - We prevent data leakage by fitting the scaler only on training data\n",
    "    - The test set remains truly unseen data, transformed using statistics from the training set\n",
    "    - This better mimics real-world scenarios where we'd need to transform new, unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d7d57a-ecf7-4922-8655-e71e8aee1975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "105cb3d8-90c6-4ab5-a055-7acc3dcf6b83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Data Generation Function\n",
    "# -------------------------\n",
    "def generate_regression_data(n_samples=1000, n_features=1, noise=20.0, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic regression data using sklearn's make_regression.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples to generate\n",
    "    n_features : int\n",
    "        Number of features (independent variables)\n",
    "    noise : float\n",
    "        Standard deviation of gaussian noise\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : ndarray of shape (n_samples, n_features)\n",
    "        Generated samples\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        Target values\n",
    "    \"\"\"\n",
    "    X, y = make_regression(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        noise=noise,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Reshape y to be a column vector\n",
    "    y = y.reshape(-1, 1)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36720c1b-9270-44ae-ae92-8f7c96e67192",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (10000, 3)\n",
      "Target shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Generate random data\n",
    "X, y = generate_regression_data(n_samples=10000, n_features=3, random_state=101)\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a4e8a8e-e1f4-4121-a262-ca0ec12125aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the data description \n",
    "df = pd.DataFrame(data = np.concatenate([X, y], axis = 1),\n",
    "                  columns = [f\"X_{i}\" for i in range(1,4)]+['Y']\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fd489f3-4405-474b-b8c7-7b3c1fc875fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X_1</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>-0.001262</td>\n",
       "      <td>0.994194</td>\n",
       "      <td>-3.806886</td>\n",
       "      <td>-0.674393</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.676238</td>\n",
       "      <td>4.155123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X_2</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>-0.004265</td>\n",
       "      <td>1.007255</td>\n",
       "      <td>-3.756504</td>\n",
       "      <td>-0.680627</td>\n",
       "      <td>-0.008693</td>\n",
       "      <td>0.678417</td>\n",
       "      <td>4.651961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X_3</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.006563</td>\n",
       "      <td>0.996595</td>\n",
       "      <td>-3.919881</td>\n",
       "      <td>-0.665755</td>\n",
       "      <td>0.005914</td>\n",
       "      <td>0.666455</td>\n",
       "      <td>4.260621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.453045</td>\n",
       "      <td>110.003277</td>\n",
       "      <td>-444.537202</td>\n",
       "      <td>-74.224674</td>\n",
       "      <td>-0.384304</td>\n",
       "      <td>74.285186</td>\n",
       "      <td>403.351975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count      mean         std         min        25%       50%  \\\n",
       "X_1  10000.0 -0.001262    0.994194   -3.806886  -0.674393  0.001271   \n",
       "X_2  10000.0 -0.004265    1.007255   -3.756504  -0.680627 -0.008693   \n",
       "X_3  10000.0  0.006563    0.996595   -3.919881  -0.665755  0.005914   \n",
       "Y    10000.0  0.453045  110.003277 -444.537202 -74.224674 -0.384304   \n",
       "\n",
       "           75%         max  \n",
       "X_1   0.676238    4.155123  \n",
       "X_2   0.678417    4.651961  \n",
       "X_3   0.666455    4.260621  \n",
       "Y    74.285186  403.351975  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c12c411-2a26-43b6-91f4-6e3db9737ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF-GPU:2.16",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
