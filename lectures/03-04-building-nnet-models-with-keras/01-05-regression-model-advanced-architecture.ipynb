{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50026306-2640-4e16-9bf1-bec66bf41320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ======================================================================= #\n",
    "# Course: Deep Learning Complete Course (CS-501)\n",
    "# Author: Dr. Saad Laouadi\n",
    "# Lesson: Deep Learning Regression Tutorial\n",
    "#\n",
    "# Description: Training Linear Regression with Keras 3 API\n",
    "#    \"\"\"\n",
    "#    Project Description:\n",
    "#    ------------------\n",
    "#    This notebook demonstrates how to build a deep learning regression model using TensorFlow/Keras.\n",
    "#    We'll generate synthetic data using scikit-learn, then build, train, and evaluate a neural network\n",
    "#    for regression tasks. This tutorial is designed for educational purposes to help understand the\n",
    "#    complete workflow of creating deep learning models for regression problems.\n",
    "#\n",
    "#    Objectives:\n",
    "#    ----------\n",
    "#    1. Learn how to generate synthetic regression data\n",
    "#    2. Understand deep learning model architecture for regression\n",
    "#    3. Learn the proper steps for data preprocessing\n",
    "#    4. Build and compile a neural network using Keras\n",
    "#    5. Train and evaluate the model's performance\n",
    "#    6. Visualize the results and model predictions\n",
    "#    \"\"\"\n",
    "# =======================================================================\n",
    "#.          Copyright © Dr. Saad Laouadi 2024\n",
    "# ======================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "47f0ec76-332c-40d3-a330-446845703d34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "Author: Dr. Saad Laouadi\n",
      "\n",
      "Last updated: 2024-11-30\n",
      "\n",
      "Compiler    : Clang 14.0.6 \n",
      "OS          : Darwin\n",
      "Release     : 24.1.0\n",
      "Machine     : arm64\n",
      "Processor   : arm\n",
      "CPU cores   : 16\n",
      "Architecture: 64bit\n",
      "\n",
      "========================================================================\n",
      "Imported Packages and Their Versions:\n",
      "========================================================================\n",
      "numpy     : 1.26.4\n",
      "tensorflow: 2.16.2\n",
      "matplotlib: 3.9.2\n",
      "sklearn   : 1.5.1\n",
      "sys       : 3.11.10 (main, Oct  3 2024, 02:26:51) [Clang 14.0.6 ]\n",
      "keras     : 3.6.0\n",
      "pandas    : 2.2.2\n",
      "\n",
      "========================================================================\n"
     ]
    }
   ],
   "source": [
    "# 1. Environment Setup\n",
    "# ------------------\n",
    "import os  \n",
    "import sys \n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "# Disable Metal API Validation\n",
    "os.environ[\"METAL_DEVICE_WRAPPER_TYPE\"] = \"0\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, root_mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization, Activation, Add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "print(\"=\"*72)\n",
    "\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Dr. Saad Laouadi\" -u -d -m\n",
    "\n",
    "print(\"=\"*72)\n",
    "print(\"Imported Packages and Their Versions:\")\n",
    "print(\"=\"*72)\n",
    "\n",
    "%watermark -iv\n",
    "print(\"=\"*72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "105cb3d8-90c6-4ab5-a055-7acc3dcf6b83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Data Generation Function\n",
    "# -------------------------\n",
    "def generate_regression_data(n_samples=1000, n_features=1, noise=20.0, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic regression data using sklearn's make_regression.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples to generate\n",
    "    n_features : int\n",
    "        Number of features (independent variables)\n",
    "    noise : float\n",
    "        Standard deviation of gaussian noise\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : ndarray of shape (n_samples, n_features)\n",
    "        Generated samples\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        Target values\n",
    "    \"\"\"\n",
    "    X, y = make_regression(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        noise=noise,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Reshape y to be a column vector\n",
    "    y = y.reshape(-1, 1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Normalize the data\n",
    "def normalize_data(df):\n",
    "    \"\"\"\n",
    "    Normalize the features and target using StandardScaler.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        DataFrame containing features and target\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_scaled : numpy array\n",
    "        Normalized features\n",
    "    y_scaled : numpy array\n",
    "        Normalized target\n",
    "    scalers : tuple\n",
    "        (X_scaler, y_scaler) for inverse transformation if needed\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X_data = df.drop('Y', axis=1)\n",
    "    y_data = df['Y']\n",
    "    \n",
    "    # Create scalers\n",
    "    X_scaler = StandardScaler()\n",
    "    y_scaler = StandardScaler()\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    X_scaled = X_scaler.fit_transform(X_data)\n",
    "    y_scaled = y_scaler.fit_transform(y_data.values.reshape(-1, 1))\n",
    "    \n",
    "    return X_scaled, y_scaled, (X_scaler, y_scaler)\n",
    "\n",
    "\n",
    "def normalize_features_split(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Normalize features using StandardScaler after splitting.\n",
    "    Fits on training data and transforms both training and test data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : numpy array or DataFrame\n",
    "        Training features\n",
    "    X_test : numpy array or DataFrame\n",
    "        Test features\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train_scaled : numpy array\n",
    "        Normalized training features\n",
    "    X_test_scaled : numpy array\n",
    "        Normalized test features\n",
    "    scaler : StandardScaler\n",
    "        Fitted scaler for future transformations\n",
    "    \"\"\"\n",
    "    # Create scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit and transform training data\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Transform test data using training fit\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "# Regression Evaluation Metrics\n",
    "def evaluate_regression_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate common regression metrics\n",
    "    \"\"\"\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "36720c1b-9270-44ae-ae92-8f7c96e67192",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (10000, 3)\n",
      "Target shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Generate random data\n",
    "X, y = generate_regression_data(n_samples=10000, n_features=3, random_state=101)\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a4e8a8e-e1f4-4121-a262-ca0ec12125aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the data description \n",
    "df = pd.DataFrame(data = np.concatenate([X, y], axis = 1),\n",
    "                  columns = [f\"X_{i}\" for i in range(1,4)]+['Y']\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5fd489f3-4405-474b-b8c7-7b3c1fc875fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X_1</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>-0.001262</td>\n",
       "      <td>0.994194</td>\n",
       "      <td>-3.806886</td>\n",
       "      <td>-0.674393</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.676238</td>\n",
       "      <td>4.155123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X_2</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>-0.004265</td>\n",
       "      <td>1.007255</td>\n",
       "      <td>-3.756504</td>\n",
       "      <td>-0.680627</td>\n",
       "      <td>-0.008693</td>\n",
       "      <td>0.678417</td>\n",
       "      <td>4.651961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X_3</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.006563</td>\n",
       "      <td>0.996595</td>\n",
       "      <td>-3.919881</td>\n",
       "      <td>-0.665755</td>\n",
       "      <td>0.005914</td>\n",
       "      <td>0.666455</td>\n",
       "      <td>4.260621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.453045</td>\n",
       "      <td>110.003277</td>\n",
       "      <td>-444.537202</td>\n",
       "      <td>-74.224674</td>\n",
       "      <td>-0.384304</td>\n",
       "      <td>74.285186</td>\n",
       "      <td>403.351975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count      mean         std         min        25%       50%  \\\n",
       "X_1  10000.0 -0.001262    0.994194   -3.806886  -0.674393  0.001271   \n",
       "X_2  10000.0 -0.004265    1.007255   -3.756504  -0.680627 -0.008693   \n",
       "X_3  10000.0  0.006563    0.996595   -3.919881  -0.665755  0.005914   \n",
       "Y    10000.0  0.453045  110.003277 -444.537202 -74.224674 -0.384304   \n",
       "\n",
       "           75%         max  \n",
       "X_1   0.676238    4.155123  \n",
       "X_2   0.678417    4.651961  \n",
       "X_3   0.666455    4.260621  \n",
       "Y    74.285186  403.351975  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74e1a632-b20b-4ddf-a1d0-63fec0e52128",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The X train set shape: (8000, 3)\n",
      "The X test set shape: (2000, 3)\n",
      "The y train set shape: (8000, 1)\n",
      "The y test set shape: (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "# First, split into train and test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.3,  # Reserve 30% for validation + test\n",
    "    random_state=101\n",
    ")\n",
    "\n",
    "# Second, split the temporary set into validation and test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.5,  # Split the remaining 30% equally\n",
    "    random_state=101\n",
    ")\n",
    "\n",
    "print(\"Training set:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set:\", X_val.shape, y_val.shape)\n",
    "print(\"Test set:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0a49b7c-2068-4da5-a99d-cad835692fee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply normalization to our split data\n",
    "X_train_scaled, X_test_scaled, scaler = normalize_features_split(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "72bd7ff5-e5cd-4669-b673-7d7d1f29d065",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Advanced Regression Model Structure\n",
    "# ---------------------------------\n",
    "\n",
    "def create_advanced_model(input_dim, \n",
    "                        dropout_rate=0.3,\n",
    "                        l2_reg=0.01):\n",
    "    \"\"\"\n",
    "    Create a more sophisticated regression model with:\n",
    "    - Wider layers at start, gradually narrowing\n",
    "    - Batch Normalization for better training\n",
    "    - Dropout for regularization\n",
    "    - L2 regularization on weights\n",
    "    - Residual connections\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input Layer\n",
    "    inputs = Input(shape=(input_dim,), name='input_layer')\n",
    "    \n",
    "    # First Block\n",
    "    x = Dense(256, kernel_regularizer=l2(l2_reg), name='dense_1')(inputs)\n",
    "    x = BatchNormalization(name='batch_norm_1')(x)\n",
    "    x = Activation('relu', name='activation_1')(x)\n",
    "    x = Dropout(dropout_rate, name='dropout_1')(x)\n",
    "    \n",
    "    # Second Block (with residual connection)\n",
    "    block_2 = Dense(256, kernel_regularizer=l2(l2_reg), name='dense_2')(x)\n",
    "    block_2 = BatchNormalization(name='batch_norm_2')(block_2)\n",
    "    block_2 = Activation('relu', name='activation_2')(block_2)\n",
    "    block_2 = Dropout(dropout_rate, name='dropout_2')(block_2)\n",
    "    # Add residual connection\n",
    "    x = Add()([x, block_2])\n",
    "    \n",
    "    # Third Block - Reducing dimensions\n",
    "    x = Dense(128, kernel_regularizer=l2(l2_reg), name='dense_3')(x)\n",
    "    x = BatchNormalization(name='batch_norm_3')(x)\n",
    "    x = Activation('relu', name='activation_3')(x)\n",
    "    x = Dropout(dropout_rate/2, name='dropout_3')(x)\n",
    "    \n",
    "    # Fourth Block - Further reduction\n",
    "    x = Dense(64, kernel_regularizer=l2(l2_reg), name='dense_4')(x)\n",
    "    x = BatchNormalization(name='batch_norm_4')(x)\n",
    "    x = Activation('relu', name='activation_4')(x)\n",
    "    x = Dropout(dropout_rate/2, name='dropout_4')(x)\n",
    "    \n",
    "    # Output Layer\n",
    "    outputs = Dense(1, name='output')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='advanced_regression')\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='advanced_regression')\n",
    "    \n",
    "    # Compile with fixed learning rate instead of schedule\n",
    "    optimizer = Adam(learning_rate=0.001)  # Fixed learning rate\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "41b0c8a5-cf61-4a55-8ed6-a35009d37cff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training with proper callbacks\n",
    "def train_advanced_model(model, X_train, y_train, X_val, y_val):\n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Reduce learning rate on plateau\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Model checkpoint\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "93b5f340-f67e-4314-b479-161ac730b95c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m248/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 10674.6094 - mae: 82.6004\n",
      "Epoch 1: val_loss improved from inf to 9316.20898, saving model to best_model.keras\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 10664.9863 - mae: 82.5653 - val_loss: 9316.2090 - val_mae: 78.6799 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m247/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7216.7607 - mae: 68.0372\n",
      "Epoch 2: val_loss improved from 9316.20898 to 3190.29395, saving model to best_model.keras\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 7199.8613 - mae: 67.9499 - val_loss: 3190.2939 - val_mae: 46.6688 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3662.6721 - mae: 47.3743\n",
      "Epoch 3: val_loss improved from 3190.29395 to 710.68909, saving model to best_model.keras\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 3660.2817 - mae: 47.3567 - val_loss: 710.6891 - val_mae: 21.2740 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m247/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1822.3418 - mae: 32.5676\n",
      "Epoch 4: val_loss improved from 710.68909 to 411.89496, saving model to best_model.keras\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1818.8591 - mae: 32.5397 - val_loss: 411.8950 - val_mae: 16.1286 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m249/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1297.6079 - mae: 27.7072\n",
      "Epoch 5: val_loss improved from 411.89496 to 411.04639, saving model to best_model.keras\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1297.2688 - mae: 27.7062 - val_loss: 411.0464 - val_mae: 15.9985 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m249/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1169.8107 - mae: 26.6902\n",
      "Epoch 6: val_loss did not improve from 411.04639\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1169.8947 - mae: 26.6912 - val_loss: 417.3547 - val_mae: 16.1568 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m248/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1278.6224 - mae: 27.5846\n",
      "Epoch 7: val_loss did not improve from 411.04639\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1277.2964 - mae: 27.5726 - val_loss: 418.7623 - val_mae: 16.1821 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m249/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1119.2437 - mae: 26.3162\n",
      "Epoch 8: val_loss did not improve from 411.04639\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1119.7609 - mae: 26.3222 - val_loss: 416.5978 - val_mae: 16.0981 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m248/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1225.3826 - mae: 27.2042\n",
      "Epoch 9: val_loss did not improve from 411.04639\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 1225.2196 - mae: 27.2048 - val_loss: 422.6508 - val_mae: 16.2116 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1084.2308 - mae: 25.9116\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 10: val_loss did not improve from 411.04639\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1084.3281 - mae: 25.9122 - val_loss: 428.3940 - val_mae: 16.4080 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m248/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1072.6600 - mae: 25.8414\n",
      "Epoch 11: val_loss improved from 411.04639 to 407.29230, saving model to best_model.keras\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1072.7128 - mae: 25.8408 - val_loss: 407.2923 - val_mae: 15.9684 - learning_rate: 2.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m247/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1142.1995 - mae: 26.5395\n",
      "Epoch 12: val_loss did not improve from 407.29230\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 1141.8365 - mae: 26.5362 - val_loss: 410.1374 - val_mae: 16.0123 - learning_rate: 2.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1188.0726 - mae: 27.0130\n",
      "Epoch 13: val_loss did not improve from 407.29230\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 1187.9388 - mae: 27.0114 - val_loss: 411.3054 - val_mae: 16.0254 - learning_rate: 2.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m249/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1171.0797 - mae: 26.7068\n",
      "Epoch 14: val_loss did not improve from 407.29230\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1170.4637 - mae: 26.7001 - val_loss: 419.0243 - val_mae: 16.0967 - learning_rate: 2.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m248/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1109.6492 - mae: 26.1087\n",
      "Epoch 15: val_loss improved from 407.29230 to 406.46432, saving model to best_model.keras\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 1109.4261 - mae: 26.1065 - val_loss: 406.4643 - val_mae: 15.9413 - learning_rate: 2.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m249/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1161.5659 - mae: 26.7266\n",
      "Epoch 16: val_loss did not improve from 406.46432\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1161.2183 - mae: 26.7230 - val_loss: 410.4790 - val_mae: 15.9833 - learning_rate: 2.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1082.3271 - mae: 25.8081\n",
      "Epoch 17: val_loss did not improve from 406.46432\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1082.4381 - mae: 25.8089 - val_loss: 408.6983 - val_mae: 15.9592 - learning_rate: 2.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m249/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1153.7129 - mae: 26.6118\n",
      "Epoch 18: val_loss did not improve from 406.46432\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1153.6244 - mae: 26.6101 - val_loss: 407.9074 - val_mae: 15.9663 - learning_rate: 2.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m247/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1054.5872 - mae: 25.5184\n",
      "Epoch 19: val_loss did not improve from 406.46432\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1054.8230 - mae: 25.5191 - val_loss: 408.4259 - val_mae: 15.9601 - learning_rate: 2.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m247/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1067.6211 - mae: 25.6354\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\n",
      "Epoch 20: val_loss did not improve from 406.46432\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1067.8292 - mae: 25.6375 - val_loss: 422.5153 - val_mae: 16.1737 - learning_rate: 2.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m247/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1151.7819 - mae: 26.5157\n",
      "Epoch 21: val_loss did not improve from 406.46432\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 1150.9111 - mae: 26.5065 - val_loss: 406.5958 - val_mae: 15.9445 - learning_rate: 4.0000e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1079.7102 - mae: 25.7621\n",
      "Epoch 22: val_loss improved from 406.46432 to 406.16812, saving model to best_model.keras\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1079.6991 - mae: 25.7620 - val_loss: 406.1681 - val_mae: 15.9393 - learning_rate: 4.0000e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1143.8655 - mae: 26.7921\n",
      "Epoch 23: val_loss did not improve from 406.16812\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 1143.8031 - mae: 26.7909 - val_loss: 406.5749 - val_mae: 15.9208 - learning_rate: 4.0000e-05\n",
      "Epoch 24/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1063.2441 - mae: 25.5890\n",
      "Epoch 24: val_loss did not improve from 406.16812\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1063.4741 - mae: 25.5920 - val_loss: 407.6401 - val_mae: 15.9098 - learning_rate: 4.0000e-05\n",
      "Epoch 25/100\n",
      "\u001b[1m248/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1035.1185 - mae: 25.2291\n",
      "Epoch 25: val_loss did not improve from 406.16812\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1035.3491 - mae: 25.2320 - val_loss: 408.7531 - val_mae: 15.9717 - learning_rate: 4.0000e-05\n",
      "Epoch 26/100\n",
      "\u001b[1m249/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1138.3799 - mae: 26.7549\n",
      "Epoch 26: val_loss did not improve from 406.16812\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1138.0734 - mae: 26.7494 - val_loss: 407.5916 - val_mae: 15.9472 - learning_rate: 4.0000e-05\n",
      "Epoch 27/100\n",
      "\u001b[1m247/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1059.9982 - mae: 25.3508\n",
      "Epoch 27: val_loss improved from 406.16812 to 405.95334, saving model to best_model.keras\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1059.7788 - mae: 25.3462 - val_loss: 405.9533 - val_mae: 15.9189 - learning_rate: 4.0000e-05\n",
      "Epoch 28/100\n",
      "\u001b[1m247/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1153.4781 - mae: 26.7927\n",
      "Epoch 28: val_loss did not improve from 405.95334\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 1153.2792 - mae: 26.7872 - val_loss: 407.2021 - val_mae: 15.9078 - learning_rate: 4.0000e-05\n",
      "Epoch 29/100\n",
      "\u001b[1m247/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1168.7939 - mae: 26.9447\n",
      "Epoch 29: val_loss did not improve from 405.95334\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1167.7157 - mae: 26.9309 - val_loss: 408.0215 - val_mae: 15.9261 - learning_rate: 4.0000e-05\n",
      "Epoch 30/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1130.7428 - mae: 26.5265\n",
      "Epoch 30: val_loss improved from 405.95334 to 405.28403, saving model to best_model.keras\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1130.7811 - mae: 26.5270 - val_loss: 405.2840 - val_mae: 15.9000 - learning_rate: 4.0000e-05\n",
      "Epoch 31/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1047.0796 - mae: 25.3050\n",
      "Epoch 31: val_loss did not improve from 405.28403\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1047.2854 - mae: 25.3075 - val_loss: 406.2993 - val_mae: 15.9478 - learning_rate: 4.0000e-05\n",
      "Epoch 32/100\n",
      "\u001b[1m248/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1075.6338 - mae: 25.7717\n",
      "Epoch 32: val_loss did not improve from 405.28403\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1075.6766 - mae: 25.7715 - val_loss: 406.5270 - val_mae: 15.9498 - learning_rate: 4.0000e-05\n",
      "Epoch 33/100\n",
      "\u001b[1m247/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1160.9557 - mae: 26.8876\n",
      "Epoch 33: val_loss improved from 405.28403 to 405.00165, saving model to best_model.keras\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1160.8995 - mae: 26.8865 - val_loss: 405.0016 - val_mae: 15.9211 - learning_rate: 4.0000e-05\n",
      "Epoch 34/100\n",
      "\u001b[1m249/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1068.2510 - mae: 25.5348\n",
      "Epoch 34: val_loss improved from 405.00165 to 404.13843, saving model to best_model.keras\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1068.4855 - mae: 25.5380 - val_loss: 404.1384 - val_mae: 15.9292 - learning_rate: 4.0000e-05\n",
      "Epoch 35/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1139.5389 - mae: 26.6136\n",
      "Epoch 35: val_loss did not improve from 404.13843\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1139.4633 - mae: 26.6125 - val_loss: 406.0591 - val_mae: 15.9098 - learning_rate: 4.0000e-05\n",
      "Epoch 36/100\n",
      "\u001b[1m248/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 998.3461 - mae: 24.5303\n",
      "Epoch 36: val_loss did not improve from 404.13843\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 999.1776 - mae: 24.5403 - val_loss: 411.1477 - val_mae: 16.0260 - learning_rate: 4.0000e-05\n",
      "Epoch 37/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1096.3800 - mae: 25.6035\n",
      "Epoch 37: val_loss did not improve from 404.13843\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 1096.4240 - mae: 25.6045 - val_loss: 405.3464 - val_mae: 15.8884 - learning_rate: 4.0000e-05\n",
      "Epoch 38/100\n",
      "\u001b[1m249/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1165.1776 - mae: 26.6855\n",
      "Epoch 38: val_loss did not improve from 404.13843\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1164.7916 - mae: 26.6821 - val_loss: 404.6236 - val_mae: 15.8853 - learning_rate: 4.0000e-05\n",
      "Epoch 39/100\n",
      "\u001b[1m248/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1011.7005 - mae: 24.7419\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\n",
      "Epoch 39: val_loss did not improve from 404.13843\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1011.7369 - mae: 24.7422 - val_loss: 405.4338 - val_mae: 15.8880 - learning_rate: 4.0000e-05\n",
      "Epoch 40/100\n",
      "\u001b[1m247/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1119.9706 - mae: 26.0665\n",
      "Epoch 40: val_loss did not improve from 404.13843\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1119.0238 - mae: 26.0570 - val_loss: 406.1036 - val_mae: 15.9089 - learning_rate: 8.0000e-06\n",
      "Epoch 41/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1123.9882 - mae: 26.3662\n",
      "Epoch 41: val_loss improved from 404.13843 to 404.09378, saving model to best_model.keras\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1123.7499 - mae: 26.3629 - val_loss: 404.0938 - val_mae: 15.8854 - learning_rate: 8.0000e-06\n",
      "Epoch 42/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1040.8289 - mae: 25.3287\n",
      "Epoch 42: val_loss did not improve from 404.09378\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1041.1388 - mae: 25.3326 - val_loss: 404.6885 - val_mae: 15.8835 - learning_rate: 8.0000e-06\n",
      "Epoch 43/100\n",
      "\u001b[1m248/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1067.5903 - mae: 25.2979\n",
      "Epoch 43: val_loss did not improve from 404.09378\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1067.2192 - mae: 25.2939 - val_loss: 405.9289 - val_mae: 15.9001 - learning_rate: 8.0000e-06\n",
      "Epoch 44/100\n",
      "\u001b[1m247/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1059.9581 - mae: 25.5662\n",
      "Epoch 44: val_loss did not improve from 404.09378\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1060.1772 - mae: 25.5656 - val_loss: 411.5100 - val_mae: 15.9996 - learning_rate: 8.0000e-06\n",
      "Epoch 45/100\n",
      "\u001b[1m249/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 977.7780 - mae: 24.3794\n",
      "Epoch 45: val_loss did not improve from 404.09378\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 978.1917 - mae: 24.3849 - val_loss: 407.8737 - val_mae: 15.9240 - learning_rate: 8.0000e-06\n",
      "Epoch 46/100\n",
      "\u001b[1m249/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1131.8364 - mae: 26.4873\n",
      "Epoch 46: val_loss improved from 404.09378 to 403.28922, saving model to best_model.keras\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 1131.6754 - mae: 26.4844 - val_loss: 403.2892 - val_mae: 15.8903 - learning_rate: 8.0000e-06\n",
      "Epoch 47/100\n",
      "\u001b[1m249/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1075.5768 - mae: 25.8037\n",
      "Epoch 47: val_loss did not improve from 403.28922\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 1075.3763 - mae: 25.8007 - val_loss: 411.2085 - val_mae: 15.9771 - learning_rate: 8.0000e-06\n",
      "Epoch 48/100\n",
      "\u001b[1m249/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1074.7906 - mae: 25.7178\n",
      "Epoch 48: val_loss did not improve from 403.28922\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 1075.2515 - mae: 25.7230 - val_loss: 405.3266 - val_mae: 15.8959 - learning_rate: 8.0000e-06\n",
      "Epoch 49/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1079.4646 - mae: 25.6497\n",
      "Epoch 49: val_loss did not improve from 403.28922\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1079.3469 - mae: 25.6487 - val_loss: 409.9188 - val_mae: 16.0173 - learning_rate: 8.0000e-06\n",
      "Epoch 50/100\n",
      "\u001b[1m247/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1086.3657 - mae: 25.8354\n",
      "Epoch 50: val_loss did not improve from 403.28922\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1086.7240 - mae: 25.8396 - val_loss: 405.2831 - val_mae: 15.8972 - learning_rate: 8.0000e-06\n",
      "Epoch 51/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1150.9490 - mae: 26.6872\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "\n",
      "Epoch 51: val_loss did not improve from 403.28922\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1150.9291 - mae: 26.6868 - val_loss: 407.9787 - val_mae: 15.9533 - learning_rate: 8.0000e-06\n",
      "Epoch 52/100\n",
      "\u001b[1m248/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1164.2006 - mae: 26.7875\n",
      "Epoch 52: val_loss did not improve from 403.28922\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1164.0338 - mae: 26.7852 - val_loss: 409.3085 - val_mae: 15.9512 - learning_rate: 1.6000e-06\n",
      "Epoch 53/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1109.7483 - mae: 25.9001\n",
      "Epoch 53: val_loss did not improve from 403.28922\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1109.7988 - mae: 25.9005 - val_loss: 407.1731 - val_mae: 15.9063 - learning_rate: 1.6000e-06\n",
      "Epoch 54/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1073.8643 - mae: 25.7769\n",
      "Epoch 54: val_loss did not improve from 403.28922\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1074.0438 - mae: 25.7783 - val_loss: 404.2669 - val_mae: 15.8788 - learning_rate: 1.6000e-06\n",
      "Epoch 55/100\n",
      "\u001b[1m249/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1162.1544 - mae: 26.6116\n",
      "Epoch 55: val_loss did not improve from 403.28922\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1161.9176 - mae: 26.6101 - val_loss: 406.1247 - val_mae: 15.9542 - learning_rate: 1.6000e-06\n",
      "Epoch 56/100\n",
      "\u001b[1m249/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1235.9231 - mae: 27.8565\n",
      "Epoch 56: val_loss improved from 403.28922 to 403.09372, saving model to best_model.keras\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 1235.4340 - mae: 27.8495 - val_loss: 403.0937 - val_mae: 15.8894 - learning_rate: 1.6000e-06\n",
      "Epoch 57/100\n",
      "\u001b[1m247/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1107.5137 - mae: 26.1161\n",
      "Epoch 57: val_loss did not improve from 403.09372\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1107.7675 - mae: 26.1168 - val_loss: 403.9690 - val_mae: 15.8862 - learning_rate: 1.6000e-06\n",
      "Epoch 58/100\n",
      "\u001b[1m249/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1100.5812 - mae: 25.9623\n",
      "Epoch 58: val_loss did not improve from 403.09372\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1100.5619 - mae: 25.9620 - val_loss: 408.8143 - val_mae: 15.9484 - learning_rate: 1.6000e-06\n",
      "Epoch 59/100\n",
      "\u001b[1m248/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1059.8287 - mae: 25.4388\n",
      "Epoch 59: val_loss did not improve from 403.09372\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1060.1816 - mae: 25.4435 - val_loss: 407.0962 - val_mae: 15.9469 - learning_rate: 1.6000e-06\n",
      "Epoch 60/100\n",
      "\u001b[1m249/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1112.5490 - mae: 26.1063\n",
      "Epoch 60: val_loss did not improve from 403.09372\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1112.5104 - mae: 26.1054 - val_loss: 406.0112 - val_mae: 15.8978 - learning_rate: 1.6000e-06\n",
      "Epoch 61/100\n",
      "\u001b[1m248/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1017.6700 - mae: 25.1119\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\n",
      "Epoch 61: val_loss did not improve from 403.09372\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1018.0233 - mae: 25.1162 - val_loss: 410.1685 - val_mae: 15.9505 - learning_rate: 1.6000e-06\n",
      "Epoch 62/100\n",
      "\u001b[1m249/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1133.3699 - mae: 26.4125\n",
      "Epoch 62: val_loss did not improve from 403.09372\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1133.1824 - mae: 26.4103 - val_loss: 404.2822 - val_mae: 15.8947 - learning_rate: 1.0000e-06\n",
      "Epoch 63/100\n",
      "\u001b[1m248/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1077.5557 - mae: 25.9690\n",
      "Epoch 63: val_loss did not improve from 403.09372\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1077.5581 - mae: 25.9658 - val_loss: 405.5025 - val_mae: 15.9329 - learning_rate: 1.0000e-06\n",
      "Epoch 64/100\n",
      "\u001b[1m247/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1020.7733 - mae: 25.0303\n",
      "Epoch 64: val_loss did not improve from 403.09372\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1021.1945 - mae: 25.0320 - val_loss: 411.3470 - val_mae: 15.9799 - learning_rate: 1.0000e-06\n",
      "Epoch 65/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1066.2943 - mae: 25.6447\n",
      "Epoch 65: val_loss did not improve from 403.09372\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1066.3384 - mae: 25.6450 - val_loss: 408.5045 - val_mae: 15.9806 - learning_rate: 1.0000e-06\n",
      "Epoch 66/100\n",
      "\u001b[1m248/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1017.2606 - mae: 24.7519\n",
      "Epoch 66: val_loss did not improve from 403.09372\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 1017.1681 - mae: 24.7525 - val_loss: 407.2537 - val_mae: 15.9105 - learning_rate: 1.0000e-06\n",
      "Epoch 67/100\n",
      "\u001b[1m247/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1115.2869 - mae: 26.4344\n",
      "Epoch 67: val_loss did not improve from 403.09372\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 1114.6403 - mae: 26.4242 - val_loss: 407.2511 - val_mae: 15.9200 - learning_rate: 1.0000e-06\n",
      "Epoch 68/100\n",
      "\u001b[1m247/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1055.3949 - mae: 25.5360\n",
      "Epoch 68: val_loss did not improve from 403.09372\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1055.2432 - mae: 25.5310 - val_loss: 405.8969 - val_mae: 15.9300 - learning_rate: 1.0000e-06\n",
      "Epoch 69/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1089.7268 - mae: 25.7868\n",
      "Epoch 69: val_loss did not improve from 403.09372\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1089.7242 - mae: 25.7866 - val_loss: 409.6841 - val_mae: 15.9390 - learning_rate: 1.0000e-06\n",
      "Epoch 70/100\n",
      "\u001b[1m248/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1041.0009 - mae: 25.3854\n",
      "Epoch 70: val_loss did not improve from 403.09372\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 1041.0741 - mae: 25.3849 - val_loss: 405.0537 - val_mae: 15.9274 - learning_rate: 1.0000e-06\n",
      "Epoch 71/100\n",
      "\u001b[1m247/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1085.9766 - mae: 25.9672\n",
      "Epoch 71: val_loss did not improve from 403.09372\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1085.8821 - mae: 25.9635 - val_loss: 406.0264 - val_mae: 15.9153 - learning_rate: 1.0000e-06\n",
      "Epoch 71: early stopping\n",
      "Restoring model weights from the end of the best epoch: 56.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = create_advanced_model(X_train_scaled.shape[1])\n",
    "history = train_advanced_model(model, X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c0c88256-12f2-4926-acdc-70c7664d2264",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 385.3754 - mae: 15.4700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[403.0937194824219, 15.889397621154785]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a38388bb-13e7-46f2-8c06-24fec05be547",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = {\n",
    "    'mae': mean_absolute_error(y_test, y_pred),\n",
    "    'rmse':root_mean_squared_error(y_test, y_pred),\n",
    "    'r2': r2_score(y_test, y_pred)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0c5929f4-7e94-472a-940f-af1496bbbf02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': 15.898946382627768,\n",
       " 'rmse': 19.93606401841813,\n",
       " 'r2': 0.9673573804059081}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e2fd7-bd54-45a9-8cea-a75f39b52037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF-GPU:2.16",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
