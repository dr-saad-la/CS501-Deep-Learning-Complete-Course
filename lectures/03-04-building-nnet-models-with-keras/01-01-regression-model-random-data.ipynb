{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50026306-2640-4e16-9bf1-bec66bf41320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ======================================================================= #\n",
    "# Course: Deep Learning Complete Course (CS-501)\n",
    "# Author: Dr. Saad Laouadi\n",
    "# Lesson: Deep Learning Regression Tutorial\n",
    "#\n",
    "# Description: Training Linear Regression with Keras 3 API\n",
    "#    \"\"\"\n",
    "#    Project Description:\n",
    "#    ------------------\n",
    "#    This notebook demonstrates how to build a deep learning regression model using TensorFlow/Keras.\n",
    "#    We'll generate synthetic data using scikit-learn, then build, train, and evaluate a neural network\n",
    "#    for regression tasks. This tutorial is designed for educational purposes to help understand the\n",
    "#    complete workflow of creating deep learning models for regression problems.\n",
    "#\n",
    "#    Objectives:\n",
    "#    ----------\n",
    "#    1. Learn how to generate synthetic regression data\n",
    "#    2. Understand deep learning model architecture for regression\n",
    "#    3. Learn the proper steps for data preprocessing\n",
    "#    4. Build and compile a neural network using Keras\n",
    "#    5. Train and evaluate the model's performance\n",
    "#    6. Visualize the results and model predictions\n",
    "#    \"\"\"\n",
    "# =======================================================================\n",
    "#.          Copyright © Dr. Saad Laouadi 2024\n",
    "# ======================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f0ec76-332c-40d3-a330-446845703d34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Environment Setup\n",
    "# ------------------\n",
    "import os  \n",
    "import sys \n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "# Disable Metal API Validation\n",
    "os.environ[\"METAL_DEVICE_WRAPPER_TYPE\"] = \"0\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "print(\"=\"*72)\n",
    "\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Dr. Saad Laouadi\" -u -d -m\n",
    "\n",
    "print(\"=\"*72)\n",
    "print(\"Imported Packages and Their Versions:\")\n",
    "print(\"=\"*72)\n",
    "\n",
    "%watermark -iv\n",
    "print(\"=\"*72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105cb3d8-90c6-4ab5-a055-7acc3dcf6b83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Data Generation Function\n",
    "# -------------------------\n",
    "def generate_regression_data(n_samples=1000, n_features=1, noise=20.0, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic regression data using sklearn's make_regression.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples to generate\n",
    "    n_features : int\n",
    "        Number of features (independent variables)\n",
    "    noise : float\n",
    "        Standard deviation of gaussian noise\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : ndarray of shape (n_samples, n_features)\n",
    "        Generated samples\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        Target values\n",
    "    \"\"\"\n",
    "    X, y = make_regression(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        noise=noise,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Reshape y to be a column vector\n",
    "    y = y.reshape(-1, 1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Normalize the data\n",
    "def normalize_data(df):\n",
    "    \"\"\"\n",
    "    Normalize the features and target using StandardScaler.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        DataFrame containing features and target\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_scaled : numpy array\n",
    "        Normalized features\n",
    "    y_scaled : numpy array\n",
    "        Normalized target\n",
    "    scalers : tuple\n",
    "        (X_scaler, y_scaler) for inverse transformation if needed\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X_data = df.drop('Y', axis=1)\n",
    "    y_data = df['Y']\n",
    "    \n",
    "    # Create scalers\n",
    "    X_scaler = StandardScaler()\n",
    "    y_scaler = StandardScaler()\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    X_scaled = X_scaler.fit_transform(X_data)\n",
    "    y_scaled = y_scaler.fit_transform(y_data.values.reshape(-1, 1))\n",
    "    \n",
    "    return X_scaled, y_scaled, (X_scaler, y_scaler)\n",
    "\n",
    "\n",
    "def normalize_features_split(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Normalize features using StandardScaler after splitting.\n",
    "    Fits on training data and transforms both training and test data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : numpy array or DataFrame\n",
    "        Training features\n",
    "    X_test : numpy array or DataFrame\n",
    "        Test features\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train_scaled : numpy array\n",
    "        Normalized training features\n",
    "    X_test_scaled : numpy array\n",
    "        Normalized test features\n",
    "    scaler : StandardScaler\n",
    "        Fitted scaler for future transformations\n",
    "    \"\"\"\n",
    "    # Create scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit and transform training data\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Transform test data using training fit\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36720c1b-9270-44ae-ae92-8f7c96e67192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate random data\n",
    "X, y = generate_regression_data(n_samples=10000, n_features=3, random_state=101)\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4e8a8e-e1f4-4121-a262-ca0ec12125aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the data description \n",
    "df = pd.DataFrame(data = np.concatenate([X, y], axis = 1),\n",
    "                  columns = [f\"X_{i}\" for i in range(1,4)]+['Y']\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd489f3-4405-474b-b8c7-7b3c1fc875fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1a632-b20b-4ddf-a1d0-63fec0e52128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=101\n",
    ")\n",
    "\n",
    "print(f\"The X train set shape: {X_train.shape}\")\n",
    "print(f\"The X test set shape: {X_test.shape}\")\n",
    "print(f\"The y train set shape: {y_train.shape}\")\n",
    "print(f\"The y test set shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4df8d4-9b9d-4924-9f04-f24ea82ae7aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c12c411-2a26-43b6-91f4-6e3db9737ee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply normalization to our split data\n",
    "X_train_scaled, X_test_scaled, scaler = normalize_features_split(X_train, X_test)\n",
    "\n",
    "# Verify the scaling\n",
    "print(\"\\nTraining set after scaling:\")\n",
    "print(\"X_train mean ≈ 0:\", np.mean(X_train_scaled, axis=0))\n",
    "print(\"X_train std ≈ 1:\", np.std(X_train_scaled, axis=0))\n",
    "\n",
    "print(\"\\nTest set after scaling:\")\n",
    "print(\"X_test mean:\", np.mean(X_test_scaled, axis=0))\n",
    "print(\"X_test std:\", np.std(X_test_scaled, axis=0))\n",
    "\n",
    "print(\"\\nTarget range (original scale):\")\n",
    "print(\"y_train min:\", np.min(y_train))\n",
    "print(\"y_train max:\", np.max(y_train))\n",
    "print(\"y_test min:\", np.min(y_test))\n",
    "print(\"y_test max:\", np.max(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e935d-eaaa-4702-bc94-12dcb5cf7a9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the Neural Network Model\n",
    "# ------------------------------\n",
    "\n",
    "\"\"\"\n",
    "Step-by-step building of a neural network for regression using the add() method.\n",
    "This tutorial assumes we have our scaled data: X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\"\"\"\n",
    "\n",
    "# 1. Create an empty sequential model\n",
    "model = Sequential(name = \"RegressionModel\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10966031-3a5c-4880-a6e9-450f5e7980e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1]             # Number of input features\n",
    "\n",
    "# 2. Add the Input layer first \n",
    "model.add(Input(shape=(input_shape,), name='The_Input_Layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96c2aa8-a74e-4e91-90a7-69ed7c17ed33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the model specifications\n",
    "print(\"*\"*80)\n",
    "print(\"The model Configuration\".center(80))\n",
    "print(\"*\"*80)\n",
    "\n",
    "pprint(model.get_config())\n",
    "\n",
    "print(\"The model Weights\".center(80))\n",
    "print(\"*\"*80)\n",
    "\n",
    "pprint(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5824b4c1-a274-4ed5-a65b-7362e66c3fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Add the Input Layer\n",
    "#     The first layer needs to know the input shape (number of features)\n",
    "#     units=64 means 64 neurons in this layer\n",
    "#    'relu' is a common activation function that helps the model learn non-linear patterns\n",
    "\n",
    "model.add(Dense(\n",
    "    units=64,                                  # Number of neurons in this layer\n",
    "    activation='relu',                         # Activation function\n",
    "    name='The_First_Hidden_Layer'                     # Name to identify the layer\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b85fc2-ca8d-4b19-9fbb-bfb57eca03c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"The model Weights\".center(80))\n",
    "print(\"*\"*80)\n",
    "\n",
    "pprint(model.get_weights())\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(\"The weights shape:\", model.get_weights()[0].shape)\n",
    "print(\"The bias shape:\", model.get_weights()[1].shape)\n",
    "print(\"*\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca6cf84-57c3-421d-a70b-a96c1e13ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Add First Hidden Layer\n",
    "# This layer gets its input shape automatically from the previous layer\n",
    "model.add(Dense(\n",
    "    units=32,                           # Number of neurons (smaller than previous layer)\n",
    "    activation='relu',                  # Same activation as before\n",
    "    name='hidden_layer_2'               # Name for identification\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a78702e-9c21-47e2-90ab-860a77f7e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Add Second Hidden Layer\n",
    "# Making the network \"deeper\" by adding another layer\n",
    "model.add(Dense(\n",
    "    units=16,                         # Even fewer neurons\n",
    "    activation='relu',                # Same activation\n",
    "    name='hidden_layer_3'            # Name for identification\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b821d3a5-df78-4be0-aabd-56b469e66306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Add Output Layer\n",
    "# For regression, we use 1 neuron and no activation function\n",
    "model.add(Dense(\n",
    "    units=1,                           # One neuron for regression\n",
    "    activation=None,                   # No activation for regression\n",
    "    name='output_layer'                # Name for identification\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961720e7-3c5a-47da-a4b5-d004f3cdc76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Compile the model\n",
    "# This sets up the model for training\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),  # Adam optimizer with default learning rate\n",
    "    loss='mean_squared_error',            # MSE loss for regression\n",
    "    metrics=['mae']                       # Mean Absolute Error as additional metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1e0690-1366-4ad4-9715-b1fcc3174e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Print model summary to see the architecture\n",
    "print(\"\\nModel Architecture Summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71091f57-4ed4-4859-915e-f2346bd3ea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Train the model\n",
    "print(\"\\nTraining the model...\")\n",
    "history = model.fit(\n",
    "    X_train_scaled,                     # Scaled input features\n",
    "    y_train,                            # Target values (not scaled)\n",
    "    epochs=50,                          # Number of training iterations\n",
    "    batch_size=32,                      # Samples per training iteration\n",
    "    validation_split=0.2,               # Use 20% of training data for validation\n",
    "    verbose=1                           # Show training progress\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8672ab3-2b55-47ca-bd14-a20a8edc1d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Evaluate the model on test data\n",
    "print(\"\\nEvaluating the model on test data:\")\n",
    "test_loss, test_mae = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"{'Test Loss (MSE)':<25}: {test_loss:.4f}\")\n",
    "print(f\"{'Test MAE':<25}: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8105cbd-bd15-45b0-b3e7-b41ac4610d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Make predictions\n",
    "predictions = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeac6408-741b-4291-a767-3a7f0f0c455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Visualize Results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, predictions, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8449dca7-70c1-452a-8f37-02bf83e5ab6f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b7089d-1f4e-42dd-8a00-a4c9ee4b2c74",
   "metadata": {},
   "source": [
    "# Using a List of Layer Syntax with `Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884a6b7e-6565-4a72-aeae-5409748675bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b972c9c-c3c1-4e93-91f8-63cb57852e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Create the input layer first\n",
    "input_layer = Input(shape=(X_train.shape[1],), name='input_layer')\n",
    "\n",
    "# 2. Create an empty sequential model\n",
    "model = Sequential([\n",
    "    # Start with the Input layer\n",
    "    input_layer,\n",
    "    \n",
    "    # 3. First Dense Layer (previously called input layer)\n",
    "    Dense(\n",
    "        units=64,                     # Number of neurons in this layer\n",
    "        activation='relu',            # Activation function\n",
    "        name='dense_layer_1'          # Name to identify the layer\n",
    "    ),\n",
    "    \n",
    "    # 4. First Hidden Layer\n",
    "    Dense(\n",
    "        units=32,                     # Number of neurons (smaller than previous layer)\n",
    "        activation='relu',            # Same activation as before\n",
    "        name='hidden_layer_1'         # Name for identification\n",
    "    ),\n",
    "    \n",
    "    # 5. Second Hidden Layer\n",
    "    Dense(\n",
    "        units=16,                     # Even fewer neurons\n",
    "        activation='relu',            # Same activation\n",
    "        name='hidden_layer_2'         # Name for identification\n",
    "    ),\n",
    "    \n",
    "    # 6. Output Layer\n",
    "    Dense(\n",
    "        units=1,                      # One neuron for regression\n",
    "        activation=None,              # No activation for regression\n",
    "        name='output_layer'           # Name for identification\n",
    "    )\n",
    "])\n",
    "\n",
    "# 7. Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),  # Adam optimizer with default learning rate\n",
    "    loss='mean_squared_error',            # MSE loss for regression\n",
    "    metrics=['mae']                       # Mean Absolute Error as additional metric\n",
    ")\n",
    "\n",
    "# 8. Print model summary to see the architecture\n",
    "print(\"\\nModel Architecture Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# 9. Train the model\n",
    "print(\"\\nTraining the model...\")\n",
    "history = model.fit(\n",
    "    X_train_scaled,                    # Scaled input features\n",
    "    y_train,                          # Target values (not scaled)\n",
    "    epochs=50,                        # Number of training iterations\n",
    "    batch_size=32,                    # Samples per training iteration\n",
    "    validation_split=0.2,             # Use 20% of training data for validation\n",
    "    verbose=1                         # Show training progress\n",
    ")\n",
    "\n",
    "# 10. Evaluate the model on test data\n",
    "print(\"\\nEvaluating the model on test data:\")\n",
    "test_loss, test_mae = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"Test Loss (MSE): {test_loss:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "# 11. Make predictions\n",
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# 12. Visualize Results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, predictions, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacedbfd-4ca4-4081-a5e5-1ed98b044851",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e028d9-c578-45cd-97a0-ea57da25b751",
   "metadata": {},
   "source": [
    "# Using Keras Functional API to Train Regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbad6b2-dd53-40bc-863b-bad68d51876b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the Neural Network Model using Functional API\n",
    "# -------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "Step-by-step building of a neural network for regression using the Functional API.\n",
    "This approach is more flexible and makes the model architecture more explicit.\n",
    "\"\"\"\n",
    "\n",
    "# 1. Import additional required module\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# 2. Define the Input layer\n",
    "inputs = Input(shape=(X_train.shape[1],), name='input_layer')\n",
    "\n",
    "# 3. Build the model layer by layer\n",
    "# First Dense Layer\n",
    "x = Dense(\n",
    "    units=64,\n",
    "    activation='relu',\n",
    "    name='dense_layer_1'\n",
    ")(inputs)\n",
    "\n",
    "# First Hidden Layer\n",
    "x = Dense(\n",
    "    units=32,\n",
    "    activation='relu',\n",
    "    name='hidden_layer_1'\n",
    ")(x)\n",
    "\n",
    "# Second Hidden Layer\n",
    "x = Dense(\n",
    "    units=16,\n",
    "    activation='relu',\n",
    "    name='hidden_layer_2'\n",
    ")(x)\n",
    "\n",
    "# Output Layer\n",
    "outputs = Dense(\n",
    "    units=1,\n",
    "    activation=None,\n",
    "    name='output_layer'\n",
    ")(x)\n",
    "\n",
    "# 4. Create the model by specifying inputs and outputs\n",
    "model = Model(inputs=inputs, outputs=outputs, name='regression_model')\n",
    "\n",
    "# 5. Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# 6. Print model summary\n",
    "print(\"\\nModel Architecture Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# 7. Train the model\n",
    "print(\"\\nTraining the model...\")\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 8. Evaluate and visualize results\n",
    "print(\"\\nEvaluating the model on test data:\")\n",
    "test_loss, test_mae = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"Test Loss (MSE): {test_loss:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "# 9. Make predictions\n",
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# Let's also plot the training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot predictions vs actual\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test, predictions, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Predictions vs Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d3aa88-9040-4b2d-8e79-8ac15ea27759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF-GPU:2.16",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
